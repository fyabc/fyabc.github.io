---
layout: post
title: Axial Attention
# description: 描述
permalink: /research-notes/axial-attention
categories: [tech.Attention]
---

## 链接

- 主页：<https://arxiv.org/abs/1912.12180>
- 正文：<https://arxiv.org/pdf/1912.12180.pdf>
- 附件：无
- 代码：<https://github.com/lucidrains/axial-attention> （非官方）
- 其他笔记：
  - <https://blog.csdn.net/qq_43310834/article/details/114672603>

## 信息

- Name: Axial Attention in Multidimensional Transformers
- Abbr: **Axial Attention**
- Author: Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans
- Affiliation: XXX
- Published Time: 2019.12.20
- Accept Time: 20XX.XX.XX
- Meeting: arXiv
- Citation: **70** (2021.08.29)
- Dataset: XXX

## 文章解读

正在完善中。

## 代码研究

正在完善中。

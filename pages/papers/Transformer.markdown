---
layout: post
title: Transformer细节研究
description: 描述
permalink: /research-notes/link-name
categories: [cat1, cat2, cat3]
---

## 链接

- 主页：<https://xxx>
- 正文：<https://xxx>
- 附件：<https://xxx>
- 代码：<https://xxx>
- 其他笔记：
  - <https://xxx>

## 信息

- Name: XXX
- Abbr: **XXX**
- Author: XXX
- Affiliation: XXX
- Preprint/Received Time: 20XX.XX.XX
- Published Time: 20XX.XX.XX
- Conference / Journal: XXX
- Citation: **XXX** (Check date)
- Dataset: XXX
- Main Method: XXX

## 文章解读

1. [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/01/attention.html)
2. Why PFFN implemented in two linears?
   1. An answer: <https://ai.stackexchange.com/a/17997>（从Sparse Autoencoder来的思路）

## 代码研究

正在完善中。

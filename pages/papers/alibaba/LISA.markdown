---
layout: post
title: LISA
# description: 描述
permalink: /research-notes/LISA
categories: [tech.Attention, tech.AttentionSpeedup, task.RecSys]
---

## 链接

- 主页：<https://arxiv.org/abs/2105.14068>
- 正文：<https://arxiv.org/pdf/2105.14068/pdf>
- 附件：无
- 代码：<https://xxx>
- 其他笔记：
  - <https://xxx>

## 信息

- Name: Linear-Time Self Attention with Codeword Histogram for Efficient Recommendation
- Abbr: **LISA**
- Author: Yongji Wu, Defu Lian, Neil Zhenqiang Gong, Lu Yin, Mingyang Yin, Jingren Zhou, Hongxia Yang
- Affiliation: DAMO Alibaba
- Preprint/Received Time: 20XX.XX.XX
- Published Time: 2021.05.28
- Meeting: WWW 2021
- Citation: **1** (2021.09.03)
- Dataset: XXX
- Main Method: XXX

## 文章解读

### 摘要

由于其有效性，自注意力在从自然语言处理到推荐的各种序列建模任务中变得越来越流行。
然而，自注意力受到二次计算和记忆复杂性的影响，禁止其在长序列上的应用。解决这个问题的现有方法主要依赖于稀疏注意力上下文，要么使用局部窗口，要么使用局部敏感哈希（LSH）或排序获得的置换桶，而关键信息可能会丢失。
受到使用簇质心来近似项目的矢量量化思想的启发，我们提出了 LISA（线性时间自注意力），它既享有普通自注意力的有效性，又具有稀疏注意力的效率。
LISA 与序列长度成线性关系，同时通过计算码字分布的可微直方图实现完全上下文关注。
同时，与一些有效的注意力方法不同，我们的方法对随意掩码或序列长度没有限制。我们在四个真实世界的数据集上评估我们的方法以进行顺序推荐。
结果表明，LISA 在性能和速度上都优于最先进的高效注意力方法；并且它比普通的 self-attention 快 57 倍，内存效率高 78 倍。

### 简介

自从在 Transformers [34] 中引入 self-attention 机制以来，它在各种领域的各种序列建模任务中取得了令人难以置信的成功，例如机器翻译 [4]、对象检测 [38]、音乐生成 [16] 和生物信息学 [26]。
最近，self-attention 在推荐 [18, 31, 46] 中也展示了其强大的力量。

然而，尽管由于其识别输入序列中元素之间复杂依赖关系的能力而获得了令人印象深刻的性能，但基于自注意力的模型在面对更长的序列时会遭受计算和内存成本飙升的困扰。
由于为每个标记计算整个序列的注意力分数，自注意力需要 O(L^2) 次操作来处理长度为 L 的输入序列。 这阻碍了在许多设置中建立在自注意力上的模型的可扩展性。

最近，已经提出了许多解决方案来解决这个问题。这些方法中的大多数 [1, 2, 6, 19, 28, 33, 44] 利用稀疏注意力模式，限制了每个查询可以关注的键数。虽然这些稀疏模式可以通过各种依赖于内容的方式来建立，比如 LSH [19]、排序 [33] 和 k 均值聚类 [28]，但通过裁剪每个查询的感受野可能会丢失关键信息。虽然成功地将计算注意力权重的成本从 O(L^2 D) 降低到 O(LBD)，其中 B 是固定的桶大小，但将键/值分配到桶中会产生额外的成本。该成本通常仍然是关于 L 的二次方，并且可能会导致处理较短序列的显着开销。我们观察到，在长度为 128 的序列上，Reformer [19] 可能比普通 Transformer 慢 7.6 倍。其他技术也被用来提高自注意力的效率。例如，[37] 中使用了注意力权重矩阵的低秩近似。然而，这种方法只支持双向注意力模式，并假设输入序列的长度是固定的。

我们观察到自注意力本质上计算每个查询的输入序列的加权平均值，权重是根据查询和键之间的内积计算的。对于每个查询，内积较大的键将得到更多关注。我们将此与最大内积搜索 (MIPS) 问题联系起来。 MIPS 问题在许多机器学习问题中具有重要意义 [11, 21, 29]，研究人员对快速近似 MIPS 算法进行了深入研究。其中，向量（乘积）量化[8, 13, 14]一直是一种流行且成功的方法。有了矢量量化，我们不再需要详尽地计算给定查询与数据库中所有 N 个点之间的内积。我们只能计算 B 个质心（即代码字），其中 B 是预算超参数。因此，我们成功地避免了冗余计算，因为属于同一质心的点与查询共享相同的内积。

矢量量化的思想也被应用于压缩项目嵌入矩阵并提高推荐系统的内存和搜索效率 [5, 24]。
在最先进的轻量级推荐模型 LightRec [24] 中，使用一组 B 个可差分学习的码本对项目进行编码，每个项目由 W 个码字组成。
项目由每个码本中最相似的码字的组合表示。
因此，我们只需要存储其相应码字的索引，而不是其嵌入向量。
由于码本中的码字索引可以用 log W 位进行紧凑编码，因此存储项目表示的总体内存要求可以从 4ND 字节减少到 1/8 NB log W + 4DBW 字节 [24]。

受基于矢量量化的 MIPS 算法中可以规避冗余内积计算的好处以及使用码本量化任何嵌入矩阵的能力的启发，我们提出了 LISA（线性时间自注意力），一种基于计算码字直方图。
LISA 配备了一系列码本来对项目（或任何形式的令牌）进行编码，以类似的方式显着降低内积计算的成本。
由于每个项目（令牌）都表示为码字的组合，并且整个输入序列可以压缩为每个码本的码字直方图（如图 1 所示），因此我们本质上是对码字执行注意力。
直方图用于在 O(L) 时间内计算注意力权重矩阵。然后我们将具有注意力权重的码字合并以获得输出。
为了在单向设置中启用自注意力（即使用临时掩码 [19]），我们可以采用前缀和的机制并在序列的每个位置计算直方图。

## 代码研究

正在完善中。
